{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping basics for Playwright\n",
    "\n",
    "This notebook is a combination of small scraping techniques along with how to use Playwright. Along with the class notes, the [scraping section](https://jonathansoma.com/everything/scraping/) on my Everything I Know site might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import what you need to use Playwright, and start up a new browser to use for scraping. \n",
    "\n",
    "> If you end up opening a lot of Chromes/Chromiums, shutting down the Python kernel with the stop button is an easy way to make them go away! You'll have to re-run your notebook, but at least you won't have sixty icons in your dock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Users/wangyushuo/.pyenv/versions/3.11.8/lib/python3.11/site-packages (1.48.0)\n",
      "Requirement already satisfied: greenlet==3.1.1 in /Users/wangyushuo/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: pyee==12.0.0 in /Users/wangyushuo/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from playwright) (12.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/wangyushuo/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping by class\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-class.html using their **class name**, printing out the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' method='GET'>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/by-class.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html><head><script>\\n    const html = `\\n<h1 class=\"title\">How to Scrape Things</h1>\\n<h3 class=\"subhead\">Probably using Playwright</h3>\\n<p class=\"byline\">By Jonathan Soma</p>\\n`\\n\\nsetTimeout(() => {\\n    console.log(html)\\n    document.querySelector(\\'body\\').innerHTML = html\\n}, 250)</script>\\n</head><body>\\n\\n</body></html>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Take the HTML from currently fully loaded page\n",
    "# notice: THERE'S NO REQUESTS ANYWHERE!!!!\n",
    "soup_doc = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[43msoup_doc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      2\u001b[0m subhead \u001b[38;5;241m=\u001b[39m soup_doc\u001b[38;5;241m.\u001b[39mfind(class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubhead\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      3\u001b[0m byline \u001b[38;5;241m=\u001b[39m soup_doc\u001b[38;5;241m.\u001b[39mfind(class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbyline\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "title = soup_doc.find(class_ = 'title').text.strip()\n",
    "subhead = soup_doc.find(class_ = 'subhead').text.strip()\n",
    "byline = soup_doc.find(class_ = 'byline').text.strip()\n",
    "list = f\"Title: {title}, Subhead: {subhead}, Byline: {byline}\"\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping using a single tag\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-list.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup_doc.find('h1').text.strip()\n",
    "subhead = soup_doc.find('h3').text.strip()\n",
    "byline = soup_doc.find('p').text.strip()\n",
    "\n",
    "dict = {\n",
    "    'Title': title,\n",
    "    'Subhead': subhead,\n",
    "    'Byline': byline\n",
    "}\n",
    "\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html just like you above, but use  **wait_for** to wait for the text \"Everything has shown up\" to show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html\")\n",
    "\n",
    "# Wait for the specific text or element to appear\n",
    "await page.wait_for_selector(\"text='Everything has shown up'\")\n",
    "\n",
    "# Get the page content\n",
    "html = await page.content()\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup_doc = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "line1 = soup_doc.find('p').text.strip() \n",
    "line2 = soup_doc.find_all('p')[1].text.strip()  \n",
    "line3 = soup_doc.find_all('p')[2].text.strip() \n",
    "line4 = soup_doc.find_all('p')[3].text.strip()\n",
    "\n",
    "data_dict = {\n",
    "    line1,\n",
    "    line2,\n",
    "    line3,\n",
    "    line4\n",
    "}\n",
    "\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms\n",
    "\n",
    "Display the content of the `h1` tag on http://jonathansoma.com/columbia/interactive-scrape/inputs.html. You'll need to follow the instructions to complete the form first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "\n",
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/inputs.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dropdown value \"Open\"\n",
    "await page.select_option('select', 'Open')\n",
    "\n",
    "# Fill out the input box with \"cat\"\n",
    "await page.fill('input[id=\"best-animal\"]', 'cat')\n",
    "\n",
    "# Click the button to submit the form\n",
    "await page.click('input[id=\"submit\"]')\n",
    "\n",
    "# Wait for the page to load after form submission\n",
    "await page.wait_for_selector('h1')\n",
    "\n",
    "# Get the page content after form submission\n",
    "html = await page.content()\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup_doc = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract and display the content of the h1 tag\n",
    "h1_content = soup_doc.find('h1').text.strip()\n",
    "h1_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a single table row\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into a dictionary\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, saving the title, subhead, and byline into a single dictionary called `book`.\n",
    "\n",
    "> Don't use pandas for this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "\n",
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/single-table-row.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page content\n",
    "html = await page.content()\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup_doc = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract the row and its cells\n",
    "row = soup_doc.find('tr')\n",
    "cells = row.find_all('td')\n",
    "\n",
    "# Save the extracted content into a dictionary\n",
    "book = {\n",
    "    'title': cells[0].text.strip(),\n",
    "    'subhead': cells[1].text.strip(),\n",
    "    'byline': cells[2].text.strip()\n",
    "}\n",
    "\n",
    "# Print the dictionary\n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple table rows\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html, creating a list of dictionaries. Convert to a pandas dataframe with `pd.json_normalize`. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "\n",
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get page content and parse with BeautifulSoup\n",
    "content = await page.content()\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Find the table and extract rows\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Create a list of dictionaries for each row\n",
    "data = []\n",
    "\n",
    "for row in rows:  # Loop through all rows\n",
    "    cells = row.find_all('td')\n",
    "    if cells:  # Only process rows that have <td> elements\n",
    "        row_data = []\n",
    "        for cell in cells:\n",
    "            row_data.append(cell.text.strip())\n",
    "        data.append(row_data)\n",
    "\n",
    "# Convert the list of lists to a pandas dataframe\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping an actual table\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/the-actual-table.html using pandas' HTML reading function. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from playwright.async_api import async_playwright\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "\n",
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/the-actual-table.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Get page content\n",
    "content = await page.content()\n",
    "\n",
    "# Step 4: Use pandas to read the HTML content and extract the table\n",
    "content_io = StringIO(content)\n",
    "tables = pd.read_html(content_io)\n",
    "df = tables[0]  # there's only one table on the page\n",
    "\n",
    "# Step 5: Save the dataframe to a CSV file\n",
    "df.to_csv('output2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `html.parser` vs `html5lib`\n",
    "\n",
    "Here is some good HTML:\n",
    "\n",
    "```python\n",
    "html_good = \"\"\"\n",
    "<h1>This is a title</h1>\n",
    "<h2>This is a subhead</h2>\n",
    "<p>This is a paragraph</p>\n",
    "<p>This is another paragraph</p>\n",
    "\"\"\"\n",
    "\n",
    "Here is some bad HTML:\n",
    "    \n",
    "html_bad = \"\"\"\n",
    "<h1>This is a title\n",
    "<h2>This is a subhead\n",
    "<p>This is a paragraph\n",
    "<p>This is another paragraph\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "When you're using BeautifulSoup, you can use different parsers, including `html.parser`, `html5lib` and `lxml`. Try both the good HTML and bad HTML with each parser and use `print(soup_doc.prettify())` to view the difference.\n",
    "\n",
    "What is different about each one?\n",
    "\n",
    "> You'll need to `pip install` for both html5lib and lxml. Since you aren't important them, they're coming from BeautifulSoup, you'll need to do **Kernel > Restart** and run from the top after installing to have them work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install html5lib lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Good HTML\n",
    "html_good = \"\"\"\n",
    "<h1>This is a title</h1>\n",
    "<h2>This is a subhead</h2>\n",
    "<p>This is a paragraph</p>\n",
    "<p>This is another paragraph</p>\n",
    "\"\"\"\n",
    "\n",
    "# Bad HTML\n",
    "html_bad = \"\"\"\n",
    "<h1>This is a title\n",
    "<h2>This is a subhead\n",
    "<p>This is a paragraph\n",
    "<p>This is another paragraph\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using html.parser: \n",
    "soup_good_html_parser = BeautifulSoup(html_good, 'html.parser')\n",
    "soup_bad_html_parser = BeautifulSoup(html_bad, 'html.parser')\n",
    "\n",
    "print(\"Good HTML with html.parser:\")\n",
    "print(soup_good_html_parser.prettify())\n",
    "\n",
    "print(\"\\nBad HTML with html.parser:\")\n",
    "print(soup_bad_html_parser.prettify())\n",
    "\n",
    "\n",
    "# Using html5lib: \n",
    "soup_good_html5lib = BeautifulSoup(html_good, 'html5lib')\n",
    "soup_bad_html5lib = BeautifulSoup(html_bad, 'html5lib')\n",
    "\n",
    "print(\"\\nGood HTML with html5lib:\")\n",
    "print(soup_good_html5lib.prettify())\n",
    "\n",
    "print(\"\\nBad HTML with html5lib:\")\n",
    "print(soup_bad_html5lib.prettify())\n",
    "\n",
    "\n",
    "# Using lxml\n",
    "soup_good_lxml = BeautifulSoup(html_good, 'lxml')\n",
    "soup_bad_lxml = BeautifulSoup(html_bad, 'lxml')\n",
    "\n",
    "print(\"\\nGood HTML with lxml:\")\n",
    "print(soup_good_lxml.prettify())\n",
    "\n",
    "print(\"\\nBad HTML with lxml:\")\n",
    "print(soup_bad_lxml.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
